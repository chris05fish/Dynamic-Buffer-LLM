# dynamic_buffer_agi.py
"""
Dynamic Buffer AGI prototype (v2)
- Working memory: dynamic buffer (semantic + episodic + procedural inputs)
- Semantic memory: small graph storing entity nodes and relations (with embeddings)
- Episodic memory: session chunk summaries (generated by LLM, cached)
- Procedural traces: logs of decisions and outcomes for learning/analysis
- Memory controller: composes the working buffer for each query

Safe usage: reads OPENAI_API_KEY from environment or .env (python-dotenv).
"""

# ------------------- Quiet noisy libs early -------------------
import os
# quiet TF and tokenizers parallelism warnings (set before importing heavy libs)
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"

import time
import json
import numpy as np
from dataclasses import dataclass
from typing import List, Dict, Any, Tuple
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
import faiss
from openai import OpenAI
import networkx as nx

# ------------------- Configuration -------------------
load_dotenv()  # optional .env support

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("Set OPENAI_API_KEY in environment or .env")

# OpenAI client (new SDK)
client = OpenAI(api_key=OPENAI_API_KEY)

# Models (change to suit your account)
EMBED_MODEL = "text-embedding-3-small"
GPT_MODEL = "gpt-4o-mini"

# Local embeddings config: set True to use SentenceTransformers (no remote embedding calls)
USE_LOCAL_EMBEDDINGS = True
LOCAL_EMBED_MODEL = "all-MiniLM-L6-v2"

# Buffer and scoring params
BASE_TOP_K = 8
INITIAL_BUFFER_K = 4
EXPAND_FACTOR = 2
RECENCY_HALF_LIFE_DAYS = 30.0

# Procedural trace file
PROCEDURAL_TRACE_LOG = "procedural_traces.jsonl"

# ------------------- Utilities -------------------
def now_ts() -> float:
    return time.time()

def time_str(ts: float) -> str:
    return time.ctime(ts)

def safe_api_call(fn, *args, retries=2, backoff=1.0, **kwargs):
    last = None
    for i in range(retries + 1):
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            last = e
            if i == retries:
                raise
            time.sleep(backoff * (i + 1))

# ------------------- Embeddings -------------------
if USE_LOCAL_EMBEDDINGS:
    sbert = SentenceTransformer(LOCAL_EMBED_MODEL)

def embed_texts(texts: List[str]) -> np.ndarray:
    """Return numpy array of embeddings (n x dim)."""
    if USE_LOCAL_EMBEDDINGS:
        vecs = sbert.encode(texts, convert_to_numpy=True, show_progress_bar=False)
        return np.array(vecs, dtype=np.float32)
    # remote embeddings via OpenAI
    resp = safe_api_call(client.embeddings.create, model=EMBED_MODEL, input=texts)
    return np.array([r.embedding for r in resp.data], dtype=np.float32)

# ------------------- Vector store (FAISS) -------------------
class VectorStore:
    """
    In-memory FAISS store that keeps vectors, texts, and metadata.
    Each metadata dict will store a cached 'embedding' key for reuse.
    """
    def __init__(self, dim: int):
        self.dim = dim
        self.index = faiss.IndexFlatL2(dim)
        self.vectors = np.zeros((0, dim), dtype=np.float32)  # stacked vectors
        self.texts: List[str] = []
        self.metadatas: List[Dict[str, Any]] = []

    def add(self, vectors: np.ndarray, texts: List[str], metas: List[Dict[str, Any]]):
        assert len(vectors) == len(texts) == len(metas)
        # dimension safety
        if vectors.ndim != 2 or vectors.shape[1] != self.dim:
            raise ValueError(f"Dim mismatch adding vectors: expected (*,{self.dim}) got {vectors.shape}")
        # add to faiss
        self.index.add(np.array(vectors).astype(np.float32))
        # append to stacked vector array
        if self.vectors.shape[0] == 0:
            self.vectors = np.array(vectors).astype(np.float32)
        else:
            self.vectors = np.vstack([self.vectors, np.array(vectors).astype(np.float32)])
        # attach embeddings to metadata for reuse
        for i, m in enumerate(metas):
            m = dict(m)  # copy to be safe
            m["embedding"] = vectors[i].astype(np.float32)
            self.metadatas.append(m)
        self.texts.extend(texts)

    def search(self, vector: np.ndarray, k: int = 5):
        """Return list of dicts {text, meta, vector, distance} sorted by closeness."""
        if vector.shape[0] != self.dim:
            # if user passed 1D vector, expand
            if vector.ndim == 1 and vector.shape[0] == self.dim:
                pass
            else:
                raise ValueError(f"Query vector dim {vector.shape} != store dim {self.dim}")
        D, I = self.index.search(np.array([vector]).astype(np.float32), k)
        results = []
        for dist, idx in zip(D[0], I[0]):
            if idx < len(self.texts):
                meta = self.metadatas[idx]
                results.append({
                    "text": self.texts[idx],
                    "meta": meta,
                    "vector": self.vectors[idx],
                    "distance": float(dist)
                })
        return results

# ------------------- Semantic Memory (Graph) -------------------
class SemanticMemory:
    """
    Lightweight semantic memory using networkx graph.
    Nodes: entity ids with properties {type, text, meta, embedding}
    Edges: relationships with labels
    """
    def __init__(self):
        self.g = nx.DiGraph()
        self.entity_counter = 0

    def add_entity(self, text: str, kind: str = "concept", meta: Dict[str, Any] = None) -> str:
        eid = f"E{self.entity_counter}"
        self.entity_counter += 1
        node_meta = meta.copy() if meta else {}
        node_meta["text"] = text
        # compute and store entity embedding (cheap if local)
        try:
            emb = embed_texts([text])[0]
            node_meta["embedding"] = emb.astype(np.float32)
        except Exception:
            node_meta["embedding"] = None
        self.g.add_node(eid, **node_meta, kind=kind)
        return eid

    def add_relation(self, a: str, b: str, rel: str):
        self.g.add_edge(a, b, rel=rel)

    def find_entities_by_text(self, query: str, threshold: float = 0.75) -> List[Tuple[str, Dict]]:
        # naive text match fallback
        found = []
        ql = query.lower()
        for n, data in self.g.nodes(data=True):
            if ql in data.get("text", "").lower():
                found.append((n, data))
        return found

    def neighbors(self, eid: str, radius: int = 1) -> List[Tuple[str, Dict]]:
        nodes = []
        for n in nx.single_source_shortest_path_length(self.g, eid, cutoff=radius):
            nodes.append((n, dict(self.g.nodes[n])))
        return nodes

# ------------------- Episodic Memory (session summaries) -------------------
@dataclass
class Episode:
    id: str
    start_ts: float
    end_ts: float
    messages: List[Dict[str, Any]]
    summary: str = ""

class EpisodicMemory:
    def __init__(self):
        self.episodes: Dict[str, Episode] = {}
        self.counter = 0

    def create_episode(self, messages: List[Dict[str, Any]]) -> Episode:
        eid = f"EP{self.counter}"
        self.counter += 1
        start = messages[0]["ts"] if messages else now_ts()
        end = messages[-1]["ts"] if messages else now_ts()
        ep = Episode(id=eid, start_ts=start, end_ts=end, messages=messages, summary="")
        self.episodes[eid] = ep
        return ep

    def summarize_episode(self, episode: Episode) -> str:
        """
        Use the LLM to create a concise summary of the episode.
        Summary is cached per episode to avoid repeated LLM calls.
        """
        if episode.summary:
            return episode.summary
        text_blocks = []
        for m in episode.messages:
            text_blocks.append(f"[{m['speaker']} @ {time.ctime(m['ts'])}] {m['text']}")
        prompt = (
            "Summarize the following conversation into 2-4 concise bullet points that capture the key facts, decisions, "
            "and action items. Keep it short and factual.\n\n" + "\n".join(text_blocks)
        )
        resp = safe_api_call(client.chat.completions.create,
                             model=GPT_MODEL,
                             messages=[
                                 {"role": "system", "content": "You are a concise summarizer."},
                                 {"role": "user", "content": prompt}
                             ],
                             max_tokens=256)
        summary = resp.choices[0].message.content.strip()
        episode.summary = summary
        return summary

# ------------------- Procedural Trace Logger -------------------
class ProceduralLogger:
    def __init__(self, path: str = PROCEDURAL_TRACE_LOG):
        self.path = path

    def log(self, trace: Dict[str, Any]):
        trace["ts"] = now_ts()
        with open(self.path, "a", encoding="utf-8") as f:
            f.write(json.dumps(trace) + "\n")

# ------------------- Memory Controller -------------------
class MemoryController:
    """
    Orchestrates what to pull into the working buffer for a given query.
    It uses:
      - semantic memory (graph traversal)
      - vector store (semantic retrieval)
      - episodic summaries (recent sessions)
      - procedural hints (rules)
    Returns a working buffer: list of (score, text, meta)
    """
    def __init__(self, vector_store: VectorStore, sem_mem: SemanticMemory, epi_mem: EpisodicMemory, proc_logger: ProceduralLogger):
        self.vs = vector_store
        self.sem = sem_mem
        self.epi = epi_mem
        self.logger = proc_logger

    def score_by_recency(self, meta: Dict[str, Any]) -> float:
        now = now_ts()
        age = now - meta.get("ts", now)
        lam = np.log(2) / (RECENCY_HALF_LIFE_DAYS * 24 * 3600)
        return float(np.exp(-lam * age))

    def compose_buffer(self, user_query: str, k: int = BASE_TOP_K) -> List[Tuple[float, str, Dict[str, Any]]]:
        q_emb = embed_texts([user_query])[0].astype(np.float32)

        # 1) vector candidates
        vector_cands = self.vs.search(q_emb, k=max(k, BASE_TOP_K*2))

        def cosine(a, b):
            return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-9))

        scored = []
        for cand in vector_cands:
            txt = cand["text"]
            meta = cand["meta"]
            vec = cand.get("vector") if "vector" in cand else meta.get("embedding")
            if vec is None:
                # fallback: embed once
                vec = embed_texts([txt])[0].astype(np.float32)
            sim = cosine(q_emb, vec)
            rec = self.score_by_recency(meta)
            score = 0.7 * sim + 0.3 * rec
            scored.append((score, txt, meta))

        # 2) semantic graph entity matches (use entity embeddings if available)
        ent_matches = self.sem.find_entities_by_text(user_query)
        for eid, data in ent_matches:
            neighbors = self.sem.neighbors(eid, radius=1)
            for n, nd in neighbors:
                txt = nd.get("text", "")
                ent_emb = nd.get("embedding")
                if ent_emb is not None:
                    sim = cosine(q_emb, np.array(ent_emb, dtype=np.float32))
                    score = 0.6 + 0.4 * sim  # boost by semantic alignment
                else:
                    score = 0.6 + 0.05 * np.random.rand()
                meta = {"source": "semantic_graph", "entity": n, "text": txt, "ts": now_ts()}
                scored.append((float(score), txt, meta))

        # 3) episodic summaries - take most recent 3 episodes
        recent_episodes = sorted(self.epi.episodes.values(), key=lambda e: e.end_ts, reverse=True)[:3]
        for ep in recent_episodes:
            summary = ep.summary or self.epi.summarize_episode(ep)
            meta = {"source": "episodic", "episode_id": ep.id, "ts": ep.end_ts}
            # compute sim between query and summary using cached embedding if possible
            sim = cosine(q_emb, embed_texts([summary])[0].astype(np.float32))
            rec = np.exp(-(now_ts() - ep.end_ts) / (24*3600*RECENCY_HALF_LIFE_DAYS))
            score = 0.5 * sim + 0.5 * rec
            scored.append((float(score), summary, meta))

        # 4) dedupe & sort
        seen_texts = set()
        final = []
        for s, t, m in sorted(scored, key=lambda x: -x[0]):
            key = t.strip().lower()
            if key in seen_texts or not t.strip():
                continue
            seen_texts.add(key)
            final.append((s, t, m))
            if len(final) >= max(INITIAL_BUFFER_K, k):
                break

        # Log selection
        try:
            sel_summary = [(round(s,3), (m.get('source') if isinstance(m, dict) else 'vector')) for s,_,m in final]
        except Exception:
            sel_summary = [(round(s,3), 'unknown') for s,_,_ in final]
        trace = {"action": "compose_buffer", "query": user_query, "selected_count": len(final), "selected_summary": sel_summary, "ts": now_ts()}
        self.logger.log(trace)

        return final

# ------------------- Prompt assembly -------------------
def assemble_prompt(buffer: List[Tuple[float, str, Dict[str, Any]]], user_query: str, system_prompt: str = "You are a helpful assistant. Use the context below to answer the question."):
    context_blocks = []
    for score, txt, meta in buffer:
        src = meta.get("source") if isinstance(meta, dict) else meta.get("source", "vector")
        tag = f"[{src} | score={round(score,3)}]"
        context_blocks.append(f"{tag}\n{txt}\n---\n")
    prompt = f"{system_prompt}\n\nCONTEXT:\n{''.join(context_blocks)}\nUSER: {user_query}\n\nAnswer concisely and cite which context blocks you used."
    return prompt

# ------------------- LLM call -------------------
def call_gpt(prompt: str, max_tokens: int = 400) -> str:
    resp = safe_api_call(client.chat.completions.create,
                         model=GPT_MODEL,
                         messages=[
                             {"role":"system","content":"You are a helpful assistant."},
                             {"role":"user","content":prompt}
                         ],
                         max_tokens=max_tokens)
    return resp.choices[0].message.content.strip()

# ------------------- Example: Putting it all together -------------------
def main_demo():
    # create stores
    dim = 384 if USE_LOCAL_EMBEDDINGS else 1536
    vs = VectorStore(dim)
    sem = SemanticMemory()
    epi = EpisodicMemory()
    proc = ProceduralLogger()
    controller = MemoryController(vs, sem, epi, proc)

    # sample messages
    now = now_ts()
    messages = [
        {"id": 1, "speaker": "User", "text": "I started a project called Live App to help people find friends at events.", "ts": now - 60*60*24*10},
        {"id": 2, "speaker": "User", "text": "We discussed improving the UI for check-in flow; Alex suggested a spinner and PIN.", "ts": now - 60*60*24*9},
        {"id": 3, "speaker": "User", "text": "I uploaded a design doc about UX and a link to prototypes.", "ts": now - 60*60*24*2},
        {"id": 4, "speaker": "User", "text": "I prefer minimalist design and fast onboarding. Also I hate popups.", "ts": now - 60*60*24*1},
    ]

    # ingest chunks and cache embeddings in metas
    chunks, metas = [], []
    for m in messages:
        parts = [m["text"]] if len(m["text"].split()) < 120 else m["text"].split(". ")
        for p in parts:
            text = p.strip()
            chunks.append(text)
            metas.append({"speaker": m["speaker"], "ts": m["ts"], "orig_id": m["id"], "source": "conversation"})
    # compute embeddings once
    vecs = embed_texts(chunks)
    vs.add(vecs, chunks, metas)

    # add semantic entities (with embeddings stored in node meta)
    eid_live = sem.add_entity("Live App", kind="project", meta={"note": "project about meeting people"})
    eid_alex = sem.add_entity("Alex", kind="person")
    sem.add_relation(eid_alex, eid_live, "collaborated_on")

    # create episode and summarize (cached)
    episode = epi.create_episode(messages)
    print("[INFO] Creating episodic summary...")
    print(epi.summarize_episode(episode))

    # Query
    user_query = "What did Alex suggest for the check-in flow?"
    print("\n[USER QUERY]", user_query)
    buffer = controller.compose_buffer(user_query, k=6)
    prompt = assemble_prompt(buffer, user_query)
    print("\n[COMPOSED PROMPT SNIPPET]\n", prompt[:1000], "...\n")

    # Call model
    print("[INFO] Calling LLM...")
    answer = call_gpt(prompt)
    print("\n[LLM ANSWER]\n", answer)

    # Log procedural trace (store rating None for now)
    proc.log({"action": "user_query_result", "query": user_query, "result_snippet": answer[:200], "rating": None})

if __name__ == "__main__":
    main_demo()
