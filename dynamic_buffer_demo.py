# dynamic_buffer_agi.py
"""
Dynamic Buffer AGI prototype
- Working memory: dynamic buffer (semantic + episodic + procedural inputs)
- Semantic memory: small graph storing entity nodes and relations
- Episodic memory: session chunk summaries (generated by LLM)
- Procedural traces: logs of decisions and outcomes for learning/analysis
- Memory controller: composes the working buffer for each query

SAFE: Uses OPENAI_API_KEY from environment (or .env).
"""
import os
import time
import json
import numpy as np
from dataclasses import dataclass, field
from typing import List, Dict, Any, Tuple
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
import faiss
from openai import OpenAI
import networkx as nx

# ------------------- Configuration -------------------
load_dotenv()  # optional .env support
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("Set OPENAI_API_KEY in environment or .env")

client = OpenAI(api_key=OPENAI_API_KEY)

# Models (change to suit your account)
EMBED_MODEL = "text-embedding-3-small"
GPT_MODEL = "gpt-4o-mini"

# Local embeddings config
USE_LOCAL_EMBEDDINGS = True
LOCAL_EMBED_MODEL = "all-MiniLM-L6-v2"

# Buffer and scoring params
BASE_TOP_K = 8
INITIAL_BUFFER_K = 4
EXPAND_FACTOR = 2
RECENCY_HALF_LIFE_DAYS = 30.0

# Procedural trace file
PROCEDURAL_TRACE_LOG = "procedural_traces.jsonl"

# ------------------- Utilities -------------------
def now_ts() -> float:
    return time.time()

def time_str(ts: float) -> str:
    return time.ctime(ts)

# ------------------- Embeddings -------------------
if USE_LOCAL_EMBEDDINGS:
    sbert = SentenceTransformer(LOCAL_EMBED_MODEL)

def embed_texts(texts: List[str]) -> np.ndarray:
    """Return numpy array of embeddings (n x dim)."""
    if USE_LOCAL_EMBEDDINGS:
        vecs = sbert.encode(texts, convert_to_numpy=True, show_progress_bar=False)
        # ensure float32 for faiss
        return np.array(vecs, dtype=np.float32)
    resp = client.embeddings.create(model=EMBED_MODEL, input=texts)
    return np.array([r.embedding for r in resp.data], dtype=np.float32)

# ------------------- Vector store (FAISS) -------------------
class VectorStore:
    def __init__(self, dim: int):
        self.dim = dim
        self.index = faiss.IndexFlatL2(dim)
        self.texts: List[str] = []
        self.metadatas: List[Dict[str, Any]] = []

    def add(self, vectors: np.ndarray, texts: List[str], metas: List[Dict[str, Any]]):
        assert len(vectors) == len(texts) == len(metas)
        self.index.add(np.array(vectors).astype(np.float32))
        self.texts.extend(texts)
        self.metadatas.extend(metas)

    def search(self, vector: np.ndarray, k: int = 5):
        D, I = self.index.search(np.array([vector]).astype(np.float32), k)
        results = []
        for idx in I[0]:
            if idx < len(self.texts):
                results.append({"text": self.texts[idx], "meta": self.metadatas[idx]})
        return results

# ------------------- Semantic Memory (Graph) -------------------
class SemanticMemory:
    """
    Lightweight semantic memory using networkx graph.
    Nodes: entity ids with properties {type, text, embeddings(optional)}
    Edges: relationships with labels
    """
    def __init__(self):
        self.g = nx.DiGraph()
        self.entity_counter = 0

    def add_entity(self, text: str, kind: str = "concept", meta: Dict[str, Any] = None) -> str:
        eid = f"E{self.entity_counter}"
        self.entity_counter += 1
        self.g.add_node(eid, text=text, kind=kind, meta=meta or {})
        return eid

    def add_relation(self, a: str, b: str, rel: str):
        self.g.add_edge(a, b, rel=rel)

    def find_entities_by_text(self, query: str, threshold: float = 0.75) -> List[Tuple[str, Dict]]:
        # naive text match fallback (you can use embeddings + similarity for better results)
        found = []
        ql = query.lower()
        for n, data in self.g.nodes(data=True):
            if ql in data.get("text", "").lower():
                found.append((n, data))
        return found

    def neighbors(self, eid: str, radius: int = 1) -> List[Tuple[str, Dict]]:
        nodes = []
        for n in nx.single_source_shortest_path_length(self.g, eid, cutoff=radius):
            nodes.append((n, self.g.nodes[n]))
        return nodes

# ------------------- Episodic Memory (session summaries) -------------------
@dataclass
class Episode:
    id: str
    start_ts: float
    end_ts: float
    messages: List[Dict[str, Any]]
    summary: str = ""

class EpisodicMemory:
    def __init__(self):
        self.episodes: Dict[str, Episode] = {}
        self.counter = 0

    def create_episode(self, messages: List[Dict[str, Any]]) -> Episode:
        eid = f"EP{self.counter}"
        self.counter += 1
        start = messages[0]["ts"] if messages else now_ts()
        end = messages[-1]["ts"] if messages else now_ts()
        ep = Episode(id=eid, start_ts=start, end_ts=end, messages=messages, summary="")
        self.episodes[eid] = ep
        return ep

    def summarize_episode(self, episode: Episode) -> str:
        """
        Use the LLM to create a concise summary of the episode.
        This summary becomes the episodic memory representation.
        """
        if episode.summary:
            return episode.summary
        # assemble a short prompt to summarize the messages
        text_blocks = []
        for m in episode.messages:
            text_blocks.append(f"[{m['speaker']} @ {time.ctime(m['ts'])}] {m['text']}")
        prompt = (
            "Summarize the following conversation into 2-4 concise bullet points that capture the key facts, decisions, "
            "and action items. Keep it short and factual.\n\n" + "\n".join(text_blocks)
        )
        resp = client.chat.completions.create(
            model=GPT_MODEL,
            messages=[
                {"role": "system", "content": "You are a concise summarizer."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=256
        )
        summary = resp.choices[0].message.content.strip()
        episode.summary = summary
        return summary

# ------------------- Procedural Trace Logger -------------------
class ProceduralLogger:
    def __init__(self, path: str = PROCEDURAL_TRACE_LOG):
        self.path = path

    def log(self, trace: Dict[str, Any]):
        trace["ts"] = now_ts()
        with open(self.path, "a", encoding="utf-8") as f:
            f.write(json.dumps(trace) + "\n")

# ------------------- Memory Controller -------------------
class MemoryController:
    """
    Orchestrates what to pull into the working buffer for a given query.
    It uses:
      - semantic memory (graph traversal)
      - vector store (semantic retrieval)
      - episodic summaries (recent sessions)
      - procedural hints (rules)
    Returns a working buffer: list of (score, text, meta)
    """
    def __init__(self, vector_store: VectorStore, sem_mem: SemanticMemory, epi_mem: EpisodicMemory, proc_logger: ProceduralLogger):
        self.vs = vector_store
        self.sem = sem_mem
        self.epi = epi_mem
        self.logger = proc_logger

    def score_by_recency(self, meta: Dict[str, Any]) -> float:
        now = now_ts()
        age = now - meta.get("ts", now)
        # exponential decay
        lam = np.log(2) / (RECENCY_HALF_LIFE_DAYS * 24 * 3600)
        return np.exp(-lam * age)

    def compose_buffer(self, user_query: str, k: int = BASE_TOP_K) -> List[Tuple[float, str, Dict[str, Any]]]:
        """
        Steps:
        1) Vector lookup for semantic matches
        2) Graph lookup for entities mentioned / neighbors
        3) Pull recent episodic summaries
        4) Combine & score by sim + recency + graph proximity
        """
        q_emb = embed_texts([user_query])[0]
        # 1) vector candidates
        vector_cands = self.vs.search(q_emb, k=max(k, BASE_TOP_K*2))
        # compute similarity properly by dot product (cosine)
        def cosine(a, b):
            return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-9))

        scored = []
        # score vector candidates
        for cand in vector_cands:
            txt = cand["text"]
            meta = cand["meta"]
            vec = embed_texts([txt])[0]
            sim = cosine(q_emb, vec)
            rec = self.score_by_recency(meta)
            score = 0.7 * sim + 0.3 * rec
            scored.append((score, txt, meta))

        # 2) semantic graph entity matches
        ent_matches = self.sem.find_entities_by_text(user_query)
        # add neighbors of matches
        for eid, data in ent_matches:
            neighbors = self.sem.neighbors(eid, radius=1)
            for n, nd in neighbors:
                txt = nd.get("text", "")
                # naive weight boost for graph relevance
                score = 0.6 + 0.1 * np.random.rand()  # small random to order
                meta = {"source": "semantic_graph", "entity": n, "text": txt}
                scored.append((score, txt, meta))

        # 3) episodic summaries - take most recent 2 episodes
        recent_episodes = sorted(self.epi.episodes.values(), key=lambda e: e.end_ts, reverse=True)[:3]
        for ep in recent_episodes:
            summary = ep.summary or self.epi.summarize_episode(ep)
            meta = {"source": "episodic", "episode_id": ep.id, "ts": ep.end_ts}
            # compute sim between query and summary
            sim = cosine(q_emb, embed_texts([summary])[0])
            rec = np.exp(-(now_ts() - ep.end_ts) / (24*3600*RECENCY_HALF_LIFE_DAYS))
            score = 0.5 * sim + 0.5 * rec
            scored.append((score, summary, meta))

        # 4) de-duplicate by text and sort
        seen_texts = set()
        final = []
        for s, t, m in sorted(scored, key=lambda x: -x[0]):
            key = t.strip().lower()
            if key in seen_texts or not t.strip():
                continue
            seen_texts.add(key)
            final.append((s, t, m))
            if len(final) >= max(INITIAL_BUFFER_K, k):
                break

        # procedural log of selection
        trace = {"action": "compose_buffer", "query": user_query, "selected_count": len(final), "selected_summary": [(round(s,3), (m.get('source') if isinstance(m, dict) else 'vector')) for s,_,m in final]}
        self.logger.log(trace)

        return final

# ------------------- Prompt assembly -------------------
def assemble_prompt(buffer: List[Tuple[float, str, Dict[str, Any]]], user_query: str, system_prompt: str = "You are a helpful assistant. Use the context below to answer the question."):
    context_blocks = []
    for score, txt, meta in buffer:
        src = meta.get("source") if isinstance(meta, dict) else meta.get("source", "vector")
        tag = f"[{src} | score={round(score,3)}]"
        context_blocks.append(f"{tag}\n{txt}\n---\n")
    prompt = f"{system_prompt}\n\nCONTEXT:\n{''.join(context_blocks)}\nUSER: {user_query}\n\nAnswer concisely and cite which context blocks you used."
    return prompt

# ------------------- LLM call -------------------
def call_gpt(prompt: str, max_tokens: int = 400) -> str:
    resp = client.chat.completions.create(
        model=GPT_MODEL,
        messages=[
            {"role":"system", "content": "You are a helpful assistant."},
            {"role":"user", "content": prompt}
        ],
        max_tokens=max_tokens
    )
    return resp.choices[0].message.content.strip()

# ------------------- Example: Putting it all together -------------------
def main_demo():
    # 1) Create stores
    dim = 384 if USE_LOCAL_EMBEDDINGS else 1536
    vs = VectorStore(dim)
    sem = SemanticMemory()
    epi = EpisodicMemory()
    proc = ProceduralLogger()
    controller = MemoryController(vs, sem, epi, proc)

    # 2) Ingest a sample multi-day conversation into vector store and semantic memory
    now = now_ts()
    messages = [
        {"id": 1, "speaker": "User", "text": "I started a project called Live App to help people find friends at events.", "ts": now - 60*60*24*10},
        {"id": 2, "speaker": "User", "text": "We discussed improving the UI for check-in flow; Alex suggested a spinner and PIN.", "ts": now - 60*60*24*9},
        {"id": 3, "speaker": "User", "text": "I uploaded a design doc about UX and a link to prototypes.", "ts": now - 60*60*24*2},
        {"id": 4, "speaker": "User", "text": "I prefer minimalist design and fast onboarding. Also I hate popups.", "ts": now - 60*60*24*1},
    ]
    # ingest chunks
    chunks, metas = [], []
    for m in messages:
        parts = [m["text"]] if len(m["text"].split()) < 120 else m["text"].split(". ")
        for p in parts:
            chunks.append(p.strip())
            metas.append({"speaker": m["speaker"], "ts": m["ts"], "orig_id": m["id"], "source": "conversation"})
    vecs = embed_texts(chunks)
    vs.add(vecs, chunks, metas)

    # add semantic entities from messages
    eid_live = sem.add_entity("Live App", kind="project", meta={"note": "project about meeting people"})
    eid_alex = sem.add_entity("Alex", kind="person")
    sem.add_relation(eid_alex, eid_live, "collaborated_on")

    # create an episode (simulate a session) and summarize it
    episode = epi.create_episode(messages)
    print("[INFO] Creating episodic summary...")
    print(epi.summarize_episode(episode))

    # Query and dynamic buffer composition
    user_query = "What did Alex suggest for the check-in flow?"
    print("\n[USER QUERY]", user_query)
    buffer = controller.compose_buffer(user_query, k=6)
    prompt = assemble_prompt(buffer, user_query)
    # Show prompt snippet
    print("\n[COMPOSED PROMPT SNIPPET]\n", prompt[:1000], "...\n")
    # Call model
    print("[INFO] Calling LLM...")
    answer = call_gpt(prompt)
    print("\n[LLM ANSWER]\n", answer)

    # Example: log a procedural learning trace - record this query and whether result was correct
    proc.log({"action": "user_query_result", "query": user_query, "result_snippet": answer[:200], "rating": None})

if __name__ == "__main__":
    main_demo()
